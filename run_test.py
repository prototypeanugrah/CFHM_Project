# from xml.etree.ElementPath import prepare_descendant
import os
import pandas as pd
import argparse

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from data.mimic_image_dataset import MIMICImageDataset
import clip


def main(args):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # load the model
    model, _ = clip.load("ViT-B/32", device=device, jit=False)
    print("Loaded in CLIP model.")

    model.load_state_dict(torch.load(args.clip_model_path, map_location=device))
    model = model.to(device)

    # corpus clip embeddings
    corpus_embeddings_path = args.corpus_embeddings_path
    raw_impressions, text_embeddings = get_text_embeddings(corpus_embeddings_path)
    print("Loaded in text embeddings.")

    dataset = MIMICImageDataset(img_path=args.cxr_path)  # load in dataset
    data_loader = DataLoader(
        dataset, shuffle=False, batch_size=args.batch_size
    )  # create dataloader
    print("Loaded in dataset.")

    # select top report/sentences
    y_pred = predict(
        data_loader, text_embeddings, model, device, topk=args.topk
    )  # predict

    # save reports
    if not os.path.exists(args.out_dir):
        os.makedirs(args.out_dir)
    out_path = args.out_dir + "/generated_reports.csv"
    save_reports(y_pred, raw_impressions, out_path)


def get_text_embeddings(corpus_embeddings_path):
    """
    Get the pre-generated text embeddings and corresponding impressions

    Args:
        corpus_embeddings_path (str): Path to the corpus embeddings file

    Returns:
        raw_impressions (pd dataframe): Raw impressions (reports/sentences)
        text_embeddings (embedding tensor): Embeddings of the corpus text (reports/sentences)
    """
    # Get the pre-generated text embeddings and corresponding impressions
    (raw_impressions, text_embeddings) = torch.load(
        corpus_embeddings_path
    )  # Load the embeddings

    raw_impressions.index = range(len(raw_impressions))  # Reset the index
    return (raw_impressions, text_embeddings)


def predict(loader, text_embeddings, model, device, topk=1):
    """
    Predicts the topk reports/sentences for each image in the loader

    Args:
        loader (DataLoader): DataLoader containing images
        text_embeddings (embedding tensor): Embeddings of the corpus text (reports/sentences)
        model (clip model): CLIP model to use for prediction
        device (str): Device to run the model on
        topk (int): Number of top reports/sentences to retrieve

    Returns:
        predicted_corpus_indices (np array): Predicted indices of the topk reports/sentences
    """
    predicted_corpus_indices = torch.zeros([len(loader.dataset), topk]).to(
        device
    )  # Initialize the predicted corpus indices
    batch_index = 0
    with torch.no_grad():
        for i, data in enumerate(tqdm(loader)):
            images = data["img"].to(device)

            # predict
            image_features = model.encode_image(images)  # get image features
            image_features /= image_features.norm(
                dim=-1, keepdim=True
            )  # normalize image features
            logits = image_features @ text_embeddings.T  # get logits
            preds = torch.argsort(logits, dim=-1, descending=True)[
                :, :topk
            ]  # get topk reports

            predicted_corpus_indices[batch_index : batch_index + preds.size(0), :] = (
                preds  # update predicted corpus indices
            )

            batch_index += preds.size(0)  # update batch index
    return predicted_corpus_indices.to("cpu").numpy()


def save_reports(outputs, raw_impressions, out_path):
    """
    Save the generated reports to a csv file

    Args:
        outputs (np array): Predicted indices of the topk reports/sentences
        raw_impressions (pd dataframe): Raw impressions (reports/sentences)
        out_path (str): Path to save the generated reports
    """
    reports_list = []
    for preds in outputs:
        # convert output to a report
        report = ""
        for pred in preds:
            report += raw_impressions[pred] + " "
        reports_list.append(report)
    # write reports to a csv
    df = pd.DataFrame(reports_list)
    df.columns = ["Report Impression"]
    df.to_csv(out_path, index=False)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Select the top report/sentences based on CXR-RePaiR method"
    )
    parser.add_argument(
        "--corpus_embeddings_path",
        type=str,
        required=True,
        help="name of corpus embeddings file generated by CLIP (ending in .pt)",
    )
    parser.add_argument(
        "--clip_model_path",
        type=str,
        required=True,
        help="name of clip model state dictionary (ending in .pt)",
    )
    parser.add_argument(
        "--out_dir",
        type=str,
        required=True,
        help="directory to save outputted generated reports",
    )
    parser.add_argument(
        "--cxr_path",
        type=str,
        required=True,
        help="path of X-rays, .h5 file for MIMIC-CXR dataset",
    )
    parser.add_argument(
        "--topk",
        type=int,
        required=False,
        default=1,
        help="number of top sentences to retrieve",
    )
    parser.add_argument("--batch_size", type=int, required=False, default=4)
    args = parser.parse_args()

    main(args)

    # Code to run this script
    # python run_test.py --corpus_embeddings_name corpus_embeddings/mimic_train_embeddings.pt --clip_model_path clip_pretrained.pt --out_dir mimic_results --cxr_path bootstrap_test/cxr.h5 --topk 3
